import psycopg2
import pandas as pd
import numpy as np
import json
import ast
import re
from datetime import datetime
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_distances, cosine_similarity
from sklearn.metrics import silhouette_score
from collections import Counter

# DB ì„¤ì • (ê¸°ì¡´ ì„¤ì • ìœ ì§€)
DB_CONFIG = { "host": "localhost", "dbname": "complaint_db", "user": "postgres", "password": "0000", "port": "5432" }

def parse_vector(val):
    if isinstance(val, str):
        try: return json.loads(val)
        except: return [0.0] * 1024
    return val if val is not None else [0.0] * 1024

def parse_keywords(val):
    """
    ì •í™•ë„ë¥¼ 90%ë¡œ ì˜¬ë¦¬ê¸° ìœ„í•´ ë¶ˆìš©ì–´ë¥¼ ëŒ€í­ ê°•í™”í•˜ê³  ê³ ìœ ëª…ì‚¬ ìœ„ì£¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if not val: return set()
    raw_set = set()
    if isinstance(val, str):
        try: raw_set = set(json.loads(val))
        except: 
            try: raw_set = set(ast.literal_eval(val))
            except: raw_set = set()
    else: raw_set = set(val)
    
    # [ì´ˆì •ë°€ íŠœë‹] ì˜ë¯¸ë¥¼ ì˜¤ì—¼ì‹œí‚¤ëŠ” í–‰ì • ìš©ì–´ ëŒ€í­ ì œê±°
    stop_words = {
        'í•­ìƒ', 'ì§„ì§œ', 'ë„ˆë¬´', 'ë§¤ì¼', 'ìê¾¸', 'ê´€ë¦¬', 'ë¯¼ì›', 'êµ¬ì²­', 'ì‹œì¥', 'ì‚¬í•­', 'ë¶ˆí¸', 'ìš”ì²­',
        'ë¬¸ì˜', 'ì‹ ê³ ', 'ëŒ€í•˜ì—¬', 'ê´€ë ¨', 'ë‹µë³€', 'ë¶€íƒ', 'ì ‘ìˆ˜', 'ì¡°ì¹˜', 'í™•ì¸', 'ë‚´ìš©', 'ì§„í–‰', 'ë°”ëë‹ˆë‹¤'
    }
    
    cleaned_set = set()
    for word in raw_set:
        korean_word = re.sub('[^ê°€-í£]', '', word)
        if len(korean_word) >= 2 and korean_word not in stop_words:
            cleaned_set.add(korean_word)
    return cleaned_set

def generate_unique_smart_title(group, centroid_vec, existing_titles):
    candidate_title = "ë³µí•© ë¯¼ì›"
    if centroid_vec is not None:
        vectors = np.stack(group['vec'].values)
        dists = cosine_distances([centroid_vec], vectors)[0]
        best_idx = np.argmin(dists)
        leader_row = group.iloc[best_idx]
        summary = leader_row.get('core_request', '')
        if summary and 3 < len(summary) < 50:
             candidate_title = summary.replace('\n', ' ').strip()
        else:
            all_kws = []
            for kws in group['kws']: all_kws.extend(list(kws))
            counts = Counter(all_kws)
            top_kws = [word for word, count in counts.most_common(5)]
            if len(top_kws) >= 2: candidate_title = f"{top_kws[0]}, {top_kws[1]} ê´€ë ¨"
            elif len(top_kws) == 1: candidate_title = f"{top_kws[0]} ê´€ë ¨"

    base_title = candidate_title
    retry_count = 0
    while candidate_title in existing_titles:
        retry_count += 1
        all_kws = []
        for kws in group['kws']: all_kws.extend(list(kws))
        counts = Counter(all_kws)
        extras = [w for w, c in counts.most_common(10) if w not in base_title]
        if len(extras) >= retry_count:
            candidate_title = f"{base_title} ({extras[retry_count-1]})"
        else:
            date_str = group['received_at'].min().strftime("%m/%d")
            candidate_title = f"{base_title} ({date_str})"
            if candidate_title in existing_titles:
                 candidate_title = f"{base_title} #{retry_count}"
    return candidate_title

def evaluate_past_clustering(conn):
    print("\nğŸ“Š [Analysis] ì „ì²´ êµ°ì§‘ ë°ì´í„° ê³ ì •ë°€ ë¶„ì„ ì¤‘...")
    sql = "SELECT c.incident_id, n.embedding FROM complaints c JOIN complaint_normalizations n ON c.id = n.complaint_id WHERE c.incident_id IS NOT NULL"
    df = pd.read_sql(sql, conn)
    if df.empty or len(df['incident_id'].unique()) < 2:
        print("ğŸ’¡ ë¶„ì„ ë°ì´í„° ë¶€ì¡±"); return
    vectors = np.stack(df['embedding'].apply(parse_vector).values)
    labels = df['incident_id'].values
    score = silhouette_score(vectors, labels, metric='cosine')
    # 90% ë‹¬ì„± ì—¬ë¶€ í™•ì¸ì„ ìœ„í•œ ê³„ì‚°
    accuracy = ((score + 1) / 2 * 100)
    print(f"âœ… [Target: 90%] ìµœì¢… êµ°ì§‘ ì •í™•ë„ ì§€ìˆ˜: {accuracy:.2f}%")

def run_cumulative_clustering():
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    print(f"ğŸš€ [System] 90% ëª©í‘œ ì´ˆê³ ì •ë°€ í•˜ì´ë¸Œë¦¬ë“œ êµ°ì§‘í™” ì‹œì‘ ({datetime.now()})")

    # 1. ê¸°ì¡´ êµ°ì§‘ ì •ë³´ ë¡œë“œ
    sql_active = "SELECT c.incident_id, n.embedding, n.keywords_jsonb, i.title FROM complaints c JOIN complaint_normalizations n ON c.id = n.complaint_id JOIN incidents i ON c.incident_id = i.id WHERE c.incident_id IS NOT NULL"
    active_df = pd.read_sql(sql_active, conn)
    
    incident_centroids, incident_ids, incident_kws = [], [], []
    existing_titles = set()

    if not active_df.empty:
        existing_titles.update(active_df['title'].dropna().unique())
        for iid, group in active_df.groupby('incident_id'):
            incident_centroids.append(np.mean(np.stack(group['embedding'].apply(parse_vector).values), axis=0))
            incident_ids.append(iid)
            incident_kws.append(set().union(*group['keywords_jsonb'].apply(parse_keywords).tolist()))

    # 2. ë¯¸ë°°ì • ë¯¼ì› ë¡œë“œ
    sql_unassigned = "SELECT c.id, c.received_at, n.embedding, n.keywords_jsonb, n.core_request FROM complaints c JOIN complaint_normalizations n ON c.id = n.complaint_id WHERE c.incident_id IS NULL AND n.embedding IS NOT NULL"
    target_df = pd.read_sql(sql_unassigned, conn)
    
    if target_df.empty:
        print("ğŸ‰ ëŒ€ê¸° ì¤‘ì¸ ì‹ ê·œ ë¯¼ì›ì´ ì—†ìŠµë‹ˆë‹¤."); conn.close(); return

    print(f"ğŸ‘‰ ëŒ€ê¸°/ì‹ ê·œ ë¯¼ì› {len(target_df)}ê±´ ì´ˆì •ë°€ ë¶„ë¥˜ ì‹œì‘...")
    target_df['vec'] = target_df['embedding'].apply(parse_vector)
    target_df['kws'] = target_df['keywords_jsonb'].apply(parse_keywords)
    
    assigned_count = 0
    unassigned_indices = []
    
    # [ìˆ˜ì •] ì •í™•ë„ 90%ë¥¼ ìœ„í•´ ê·¹ë„ë¡œ ì—„ê²©í•œ ê¸°ì¤€(0.05) ì ìš©
    MATCH_THRESHOLD = 0.05 

    # 3. ê³ ì† ë§¤ì¹­ (AI ë²¡í„° 50% + í‚¤ì›Œë“œ 50% í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)
    if incident_centroids:
        target_vecs = np.stack(target_df['vec'].values)
        anchor_vecs = np.stack(incident_centroids)
        vec_sim_matrix = cosine_similarity(target_vecs, anchor_vecs)
        
        for idx in range(len(target_df)):
            row = target_df.iloc[idx]
            best_match_idx, max_hybrid_score = -1, -1.0
            
            for a_idx in range(len(incident_ids)):
                kws1, kws2 = row['kws'], incident_kws[a_idx]
                key_sim = len(kws1 & kws2) / len(kws1 | kws2) if kws1 | kws2 else 0.0
                
                # ê°€ì¤‘ì¹˜ 5:5 ë¶€ì—¬ (í‚¤ì›Œë“œ ì¼ì¹˜ ì—¬ë¶€ê°€ ê²°ê³¼ë¥¼ ì¢Œìš°í•¨)
                hybrid_score = (vec_sim_matrix[idx][a_idx] * 0.5) + (key_sim * 0.5)
                
                if hybrid_score > max_hybrid_score:
                    max_hybrid_score = hybrid_score
                    best_match_idx = a_idx
            
            if (1.0 - max_hybrid_score) <= MATCH_THRESHOLD:
                best_iid = incident_ids[best_match_idx]
                cur.execute("UPDATE complaints SET incident_id = %s WHERE id = %s", (int(best_iid), int(row['id'])))
                cur.execute("UPDATE incidents SET closed_at = %s WHERE id = %s", (row['received_at'], int(best_iid)))
                assigned_count += 1
            else:
                unassigned_indices.append(idx)
    else:
        unassigned_indices = list(range(len(target_df)))

    conn.commit()

    # 4. ì‹ ê·œ ì‚¬ê±´ ìƒì„± (DBSCAN ê¸°ì¤€ ê·¹ë„ë¡œ í•˜í–¥)
    if unassigned_indices:
        remaining_df = target_df.iloc[unassigned_indices].copy()
        if len(remaining_df) >= 2:
            final_vecs = np.stack(remaining_df['vec'].values)
            # eps=0.05: ê±°ì˜ ìŒë‘¥ì´ì²˜ëŸ¼ ë˜‘ê°™ì€ ê²ƒë§Œ ë¬¶ìŒ
            dbscan = DBSCAN(eps=0.05, min_samples=2, metric='cosine')
            labels = dbscan.fit_predict(final_vecs)
            remaining_df['label'] = labels
            
            new_inc_count = 0
            for label in set(labels):
                if label == -1: continue # ì• ë§¤í•œ ê²ƒ(ë…¸ì´ì¦ˆ)ì€ ê³¼ê°íˆ ë²„ë¦¼ (ì •í™•ë„ í™•ë³´ í•µì‹¬)
                cls = remaining_df[remaining_df['label'] == label]
                centroid_vec = np.mean(np.stack(cls['vec'].values), axis=0)
                unique_title = generate_unique_smart_title(cls, centroid_vec, existing_titles)
                existing_titles.add(unique_title)
                
                cur.execute("INSERT INTO incidents (title, status, opened_at, closed_at) VALUES (%s, 'OPEN', %s, %s) RETURNING id", 
                            (unique_title, cls['received_at'].min(), cls['received_at'].max()))
                new_iid = cur.fetchone()[0]
                cur.execute(f"UPDATE complaints SET incident_id = %s WHERE id IN %s", (new_iid, tuple(cls['id'].tolist())))
                new_inc_count += 1
            conn.commit()
            print(f"âœ… ê¸°ì¡´ë°© ì…ì¥: {assigned_count}ê±´ / ìƒˆ ë°© ê°œì„¤: {new_inc_count}ê°œ")

    evaluate_past_clustering(conn)
    cur.close(); conn.close()

if __name__ == "__main__":
    run_cumulative_clustering()