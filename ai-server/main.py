from fastapi import FastAPI
from pydantic import BaseModel
from app.services import llm_service
from app import database

app = FastAPI()

@app.post("/analyze")
async def analyze_and_store(complaint_id: int, body: str):
    # 1. LLM 요약 및 분석 (Normalization) 
    analysis = await llm_service.get_normalization(body)
    
    # 2. 벡터 추출 (Embedding) 
    embedding = await llm_service.get_embedding(analysis['neutral_summary'])
    
    # 3. DB 저장 (is_current 처리 포함 트랜잭션) 
    database.save_normalization(complaint_id, analysis, embedding)
    
    return {"status": "success", "data": analysis}

# 요청 데이터 구조 정의
class ChatRequest(BaseModel):
    query: str

# 민원 상세 화면 진입 시 (자동 분석 & 가이드)
@app.get("/api/complaints/{complaint_id}/ai-analysis")
async def get_ai_analysis(complaint_id: int):
    """
    [자동 모드]
    공무원이 민원을 클릭했을 때, DB에 있는 민원 내용을 바탕으로
    유사 사례 요약과 처리 방향 가이드를 자동으로 생성
    """
    try:
        # query 인자 없이 호출 -> llm_service 내부에서 '자동 모드'로 동작
        response = await llm_service.generate_rag_response(complaint_id)
        return {"status": "success", "result": response}
    except Exception as e:
        return {"status": "error", "message": f"AI 분석 실패: {str(e)}"}

# 챗봇에게 추가 질문하기 (Q&A)
@app.post("/api/complaints/{complaint_id}/chat")
async def chat_with_ai(complaint_id: int, request: ChatRequest):
    """
    [수동 모드]
    공무원이 채팅창에 질문(query)을 입력하면,
    해당 질문을 법률 용어로 변환 후 검색하여 답변
    """
    try:
        # query 인자 포함 호출 -> llm_service 내부에서 '수동 질문 모드'로 동작
        response = await llm_service.generate_rag_response(complaint_id, request.query)
        return {"status": "success", "result": response}
    except Exception as e:
        return {"status": "error", "message": f"답변 생성 실패: {str(e)}"}