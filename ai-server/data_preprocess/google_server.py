import ast
import os
import numpy as np
import pandas as pd
import psycopg2
from psycopg2.extras import Json
import requests
from datetime import datetime

# CSV ë°ì´í„°ë¥¼ DBë¡œ ì´ê´€í•˜ëŠ” ì½”ë“œ

DB_CONFIG = {
    "host": "0.0.0.0",
    "database": "postgres",
    "user": "postgres",
    "password": "0000",
    "port": 5432
}

OLLAMA_URL = "http://localhost:11434/api/embeddings"
EMBED_MODEL = "mxbai-embed-large"
base_path = os.path.dirname(os.path.abspath(__file__))
CSV_FILE = os.path.join(base_path, "ê°•ë™êµ¬_structured_final.csv")
TABLE_NAME = "complaint_normalizations"

# ì„ë² ë”© ì‹¤í–‰ í•¨ìˆ˜
def get_embedding(text):
    payload = {"model": EMBED_MODEL, "prompt": f"doc: {text}"}
    try:
        res = requests.post(OLLAMA_URL, json=payload, timeout=10)
        return res.json()['embedding']
    except Exception as e:
        print(f"Embedding Error: {e}")
        return None

# í‚¤ì›Œë“œì˜ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì‚­ì œ
def clean_keywords(raw_value):
    if pd.isna(raw_value) or str(raw_value).strip() == "":
        return []
    try:
        return ast.literal_eval(str(raw_value))
    except (ValueError, SyntaxError):
        return [k.strip() for k in str(raw_value).split(',')]

# ë°ì´í„°ë¥¼ DBë¡œ ì´ê´€
def migrate_data():
    try:
        df = pd.read_csv(CSV_FILE, encoding='utf-8-sig')
    except:
        df = pd.read_csv(CSV_FILE, encoding='cp949')

    # CSV ì½ì„ ë•Œ ë‚ ì§œ ë³€í™˜ ë¯¸ë¦¬ ì ìš©
    df['req_date'] = pd.to_datetime(df['req_date'], errors='coerce')
    df['resp_date'] = pd.to_datetime(df['resp_date'], errors='coerce')

    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()

    cur.execute(f"SELECT COUNT(*) FROM {TABLE_NAME}")
    last_count = cur.fetchone()[0]
    
    print(f"í˜„ì¬ DB({TABLE_NAME})ì— ì €ì¥ëœ ë°ì´í„° ìˆ˜: {last_count}ê±´")

    df_to_process = df.iloc[last_count:]
    df_to_process = df_to_process.replace({np.nan: None})
    
    if len(df_to_process) == 0:
        print("âœ¨ ì´ë¯¸ ëª¨ë“  ë°ì´í„°ê°€ ì´ê´€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸš€ ì´ {len(df)}ê±´ ì¤‘ {last_count}ê±´ ì´í›„ì¸ {len(df_to_process)}ê±´ë¶€í„° ì´ê´€ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    for i, row in df_to_process.iterrows():
        try:
            req_time = row['req_date']
            resp_time = row['resp_date'] if pd.notnull(row['resp_date']) else None
            
            status = 'CLOSED' if resp_time else 'RECEIVED'
            
            sql_parent = """
            INSERT INTO complaints (
                received_at, title, body, answer, district_id, status, address_text, 
                created_at, updated_at, closed_at, 
                current_department_id, applicant_id, tag
            ) VALUES (%s, %s, %s, %s, 2, %s, %s, %s, %s, %s, 3, 1, 'OTHER') RETURNING id;
            """
            
            cur.execute(sql_parent, (
                req_time,               
                row['req_title'], 
                row['req_content'], 
                row['resp_content'],
                status,                 
                row['resp_dept'],       
                req_time,              
                resp_time if resp_time else req_time,
                resp_time              
            ))
            new_complaint_id = cur.fetchone()[0]

            vector = get_embedding(row['search_text'])
            if not vector:
                print(f"âš ï¸ [{i}] ì„ë² ë”© ì‹¤íŒ¨ - ì´ í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                conn.rollback()
                continue

            sql_child = """
            INSERT INTO complaint_normalizations (
                complaint_id, neutral_summary, core_request, 
                target_object, keywords_jsonb, embedding, resp_dept,
                created_at  -- â˜… ë‚ ì§œ ì»¬ëŸ¼ ì¶”ê°€
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            keywords_list = clean_keywords(row['keywords'])
            
            cur.execute(sql_child, (
                new_complaint_id,
                row['search_text'],
                row['topic'],
                row['category'],
                Json(keywords_list),
                vector,
                row['resp_dept'],
                req_time
            ))

            conn.commit()
            
            if (i + 1) % 10 == 0 or i == len(df) - 1:
                print(f"âœ… [{i+1}/{len(df)}] ì´ê´€ ì™„ë£Œ (ID: {new_complaint_id}) - ë‚ ì§œ: {req_time}")

        except Exception as e:
            conn.rollback()
            print(f"âŒ Error at row {i}: {e}")
            break 

    cur.close()
    conn.close()
    print("âœ¨ ì´ê´€ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")

if __name__ == "__main__":
    migrate_data()
