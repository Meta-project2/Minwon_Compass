import os
import pandas as pd
from sqlalchemy import create_engine, text
from ragas import evaluate
from ragas.metrics import Faithfulness, AnswerRelevancy, ContextRecall
from ragas.llms import llm_factory
from ragas.embeddings import embedding_factory
from openai import OpenAI
from datasets import Dataset

# ==========================================
# 1. í™˜ê²½ ì„¤ì • ë° API í‚¤
# ==========================================
os.environ["OPENAI_API_KEY"] = "" # ì‹¤ì œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.

DB_CONFIG = {
    "host": "34.50.48.38",
    "database": "postgres",
    "user": "postgres",
    "password": "0000",
    "port": 5432
}

# ==========================================
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==========================================
def format_to_sentence(data):
    """JSON ë°ì´í„°ë¥¼ Ragas í‰ê°€ìš© ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜ (ì •ë³´ ë°€ë„ ì¼ì¹˜í™”)"""
    return (
        f"ì†Œê´€ ë¶€ì„œ: [{data['dept']}], "
        f"ì‚¬ë¡€ ìš”ì•½: {data['summary']}, "
        f"í•µì‹¬ í‚¤ì›Œë“œ: {data['keywords']}, "
        f"ë„ë©”ì¸ ì¹´í…Œê³ ë¦¬: {data['category']}"
    )

# ==========================================
# 3. ë°ì´í„° ì¶”ì¶œ ë° ê°€ê³µ í•¨ìˆ˜
# ==========================================
def get_evaluation_dataset():
    db_url = f"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
    engine = create_engine(db_url)
    
    # [í•µì‹¬] 0.6/0.2/0.2 ê°€ì¤‘ì¹˜ ë° 0.45 ì„ê³„ê°’ í•„í„°ë§ ë°˜ì˜
    query = text("""
    WITH eval_target AS (
        SELECT 
            c.id, c.body AS question,
            json_build_object(
                'dept', n.resp_dept,
                'summary', n.neutral_summary,
                'keywords', (SELECT string_agg(k, ', ') FROM jsonb_array_elements_text(n.keywords_jsonb) k),
                'category', n.target_object
            ) AS actual_data,
            n.embedding, n.keywords_jsonb, n.target_object
        FROM complaints c
        JOIN complaint_normalizations n ON c.id = n.complaint_id
        WHERE n.resp_dept IS NOT NULL
        LIMIT 30
    )
    SELECT 
        et.question,
        et.actual_data,
        (
            SELECT json_build_object(
                'dept', sub.resp_dept,
                'summary', sub.neutral_summary,
                'keywords', (SELECT string_agg(k, ', ') FROM jsonb_array_elements_text(sub.keywords_jsonb) k),
                'category', sub.target_object,
                'score', sub.final_score
            )
            FROM (
                SELECT 
                    cn.*,
                    ((1 - (cn.embedding <=> et.embedding)) * 0.6 + 
                     ts_rank(cn.search_vector, plainto_tsquery('simple', 
                         COALESCE((SELECT string_agg(k, ' ') FROM jsonb_array_elements_text(et.keywords_jsonb) k), '')::text
                     )) * 0.2 + 
                     (CASE WHEN cn.resp_dept::text LIKE '%%' || LEFT(et.target_object::text, 2) || '%%' THEN 0.2 ELSE 0 END)) AS final_score
                FROM complaint_normalizations cn
                WHERE cn.complaint_id != et.id
            ) sub
            WHERE sub.final_score > 0.45
            ORDER BY sub.final_score DESC
            LIMIT 1
        ) AS search_result
    FROM eval_target et;
    """)
    
    with engine.connect() as conn:
        df = pd.read_sql(query, conn)

    eval_rows = []
    for _, row in df.iterrows():
        actual = row['actual_data']
        predicted = row['search_result']
        
        ground_truth_sentence = format_to_sentence(actual)
        
        if predicted:
            # ì„ê³„ê°’ 0.45ë¥¼ ë„˜ëŠ” ì‚¬ë¡€ë¥¼ ì°¾ì€ ê²½ìš°
            answer_sentence = format_to_sentence(predicted)
            context_data = [str(predicted['summary'])]
        else:
            # ì„ê³„ê°’ì„ ë„˜ëŠ” ì‚¬ë¡€ê°€ ì—†ëŠ” ê²½ìš° (ì˜ˆì™¸ ì²˜ë¦¬)
            answer_sentence = "ì ì ˆí•œ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¶€ì„œ ë§¤ì¹­ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤."
            context_data = ["ê²€ìƒ‰ ì„ê³„ê°’ 0.45ë¥¼ ì´ˆê³¼í•˜ëŠ” ê´€ë ¨ ì‚¬ë¡€ ì—†ìŒ"]
            
        eval_rows.append({
            "question": str(row['question']),
            "ground_truth": ground_truth_sentence,
            "answer": answer_sentence,
            "contexts": context_data
        })
        
    return Dataset.from_pandas(pd.DataFrame(eval_rows))

# ==========================================
# 4. Ragas í‰ê°€ í•¨ìˆ˜
# ==========================================
def run_evaluation(dataset):
    openai_client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
    eval_llm = llm_factory("gpt-4o", client=openai_client)
    eval_embeddings = embedding_factory("openai", model="text-embedding-3-large", client=openai_client)

    metrics = [
        Faithfulness(llm=eval_llm),
        AnswerRelevancy(llm=eval_llm, embeddings=eval_embeddings),
        ContextRecall(llm=eval_llm)
    ]

    print("ğŸ“Š Ragas í‰ê°€ ì§€í‘œ ê³„ì‚° ì¤‘...")
    return evaluate(dataset, metrics=metrics)

# ==========================================
# 5. ì‹¤í–‰ ë©”ì¸
# ==========================================
if __name__ == "__main__":
    try:
        print("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜(0.6) ë° ì„ê³„ê°’ ì ìš© ë°ì´í„°ì…‹ ì¶”ì¶œ ì¤‘...")
        ds = get_evaluation_dataset()
        print(f"âœ… {len(ds)}ê±´ì˜ í‰ê°€ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")
        
        results = run_evaluation(ds)
        print("\nâœ¨ [ë¶€ì„œ ë§¤ì¹­ ì‹œìŠ¤í…œ ìµœì¢… ì„±ëŠ¥ ê²°ê³¼]")
        print(results)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")